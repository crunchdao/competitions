{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "845a7c99-f6c6-41e2-a924-3d63b520442c",
   "metadata": {},
   "source": [
    "# Baseline submission with the PC algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7668e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: API_BASE_URL=http://api.hub.crunchdao.io\n",
      "env: WEB_BASE_URL=http://hub.crunchdao.io\n"
     ]
    }
   ],
   "source": [
    "%env API_BASE_URL=http://api.hub.crunchdao.io\n",
    "%env WEB_BASE_URL=http://hub.crunchdao.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526ab731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the token via https://hub.crunchdao.io/competitions/causality-discovery/submit/via/notebook\n",
    "\n",
    "!crunch setup causality-discovery . --token aaaabbbbccccddddeeeeffff --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c46c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gcastle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f6e3dd2-888c-4541-b02f-d3caaae44ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 12:11:54,052 - c:\\Users\\cacer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\castle\\backend\\__init__.py[line:36] - INFO: You can use `os.environ['CASTLE_BACKEND'] = backend` to set the backend(`pytorch` or `mindspore`).\n",
      "2024-03-20 12:11:54,264 - c:\\Users\\cacer\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\castle\\algorithms\\__init__.py[line:36] - INFO: You are using ``pytorch`` as the backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is a basic example of what you need to do to participate to the tournament.\n",
    "The code will not have access to the internet (or any socket related operation).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import typing\n",
    "\n",
    "import joblib\n",
    "import crunch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import castle.algorithms\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98414dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded inline runner with module: <module '__main__'>\n"
     ]
    }
   ],
   "source": [
    "crunch = crunch.load_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23e9a77-c4f8-42b8-a0d0-27f628c89419",
   "metadata": {},
   "source": [
    "The following function is provided to help you obtaining a DAG from your predicted graph, in case it is not a DAG, also ensuring that there is an edge from X to Y, as designed. This is just one way to obtain such result and not necessarily optimal for the competition. An improved algorithm to obtain a DAG from your predicted graph could lead to better scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84842f10-13d7-40f1-8b16-a3986babbdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_DAG(g):\n",
    "    \"\"\"\n",
    "    Ensure that the graph is a DAG and has an edge X→Y\n",
    "\n",
    "    We look for cycles, and remove an edge in each cycle, until there are no cycles left.\n",
    "\n",
    "    Inputs: g: nx.DiGraph\n",
    "    Output: g: nx.DiGraph\n",
    "\n",
    "    This function provides just a possible solution to the problem\n",
    "    of DAG-ifying a graph. Other solutions can be conceived that could\n",
    "    be better for the competition.\n",
    "    \"\"\"\n",
    "\n",
    "    assert 'X' in g.nodes\n",
    "    assert 'Y' in g.nodes\n",
    "\n",
    "    gg = g.copy()\n",
    "\n",
    "    # Add X→Y if it is missing\n",
    "    if ('X', 'Y') not in gg.edges:\n",
    "        gg.add_edge( 'X', 'Y' )\n",
    "\n",
    "    # Look for cycles and remove them\n",
    "    while not nx.is_directed_acyclic_graph(gg):\n",
    "\n",
    "        h = gg.copy()\n",
    "\n",
    "        # Remove all the sources and sinks\n",
    "        while True:\n",
    "            finished = True\n",
    "            for i,v in nx.in_degree_centrality(h).items():\n",
    "                if v == 0:\n",
    "                    h.remove_node(i)\n",
    "                    finished = False\n",
    "            for i,v in nx.out_degree_centrality(h).items():\n",
    "                if v == 0:\n",
    "                    h.remove_node(i)\n",
    "                    finished = False\n",
    "            if finished:\n",
    "                break\n",
    "\n",
    "        # Find a cycle, with a random walk starting at a random node\n",
    "        node = list( h.nodes )[0]\n",
    "        cycle = [node]\n",
    "        while True:\n",
    "            edges = list( h.out_edges(node) )\n",
    "            _, node = edges[ np.random.choice( len(edges) ) ]\n",
    "            if node in cycle:\n",
    "                break\n",
    "            cycle.append( node )\n",
    "\n",
    "        # We have a path that ends with a cycle: remove the begining, if it is not part of the cycle\n",
    "        cycle = np.array(cycle)\n",
    "        i = np.argwhere( cycle == node )[0][0]\n",
    "        cycle = cycle[i:]\n",
    "        cycle = cycle.tolist() + [node]\n",
    "\n",
    "        # Edges in that cycle\n",
    "        edges = list( zip( cycle[:-1], cycle[1:] ) )\n",
    "\n",
    "        # Pick an edge at random, but make sure it is not X→Y -- we want to keep that one\n",
    "        edges = [ e for e in edges if e != ('X', 'Y') ]\n",
    "        edge = edges[ np.random.choice( len(edges) ) ]\n",
    "\n",
    "        gg.remove_edge( *edge )\n",
    "\n",
    "    return gg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db4da6e-0ef3-4e84-a432-786cac80bb8e",
   "metadata": {},
   "source": [
    "This is the core of the solution's code, that reads one dataset at the time, applies the PC algorithm, ensures that the result is a DAG, and then put the result in a single dataframe with the required format, ready for being submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cdf556e-9791-405f-a321-8a3b6931d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment what you need!\n",
    "def train(\n",
    "    X_train: typing.Dict[str, pd.DataFrame],\n",
    "    y_train: typing.Dict[str, pd.DataFrame],\n",
    "    # number_of_features: int,\n",
    "    model_directory_path: str,\n",
    "    # id_column_name: str,\n",
    "    # prediction_column_name: str,\n",
    "    # has_gpu: bool,\n",
    "    # has_trained: bool,\n",
    ") -> None:\n",
    "    model = ...\n",
    "    joblib.dump(\n",
    "        model,\n",
    "        os.path.join(model_directory_path, \"model.joblib\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf152816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment what you need!\n",
    "def infer(\n",
    "    X_test: typing.Dict[str, pd.DataFrame],\n",
    "    # number_of_features: int,\n",
    "    model_directory_path: str,\n",
    "    id_column_name: str,\n",
    "    prediction_column_name: str,\n",
    "    # has_gpu: bool,\n",
    "    # has_trained: bool,\n",
    ") -> pd.DataFrame:\n",
    "    # model = joblib.load(os.path.join(model_directory_path, \"model.joblib\"))\n",
    "\n",
    "    submission_file = {}\n",
    "    for dataset_id in tqdm(X_test):\n",
    "        print(dataset_id)\n",
    "        X = X_test[dataset_id]\n",
    "\n",
    "        nodes = X.columns\n",
    "        model = castle.algorithms.PC()\n",
    "        model.learn(X)\n",
    "\n",
    "        A_hat = pd.DataFrame(model.causal_matrix, columns=nodes, index=nodes)\n",
    "        g_hat = nx.from_pandas_adjacency(A_hat, create_using=nx.DiGraph)\n",
    "        g_hat = fix_DAG(g_hat)\n",
    "\n",
    "        G = pd.DataFrame(nx.to_numpy_array(g_hat).astype(int), columns=nodes, index=nodes)\n",
    "        for i in nodes:\n",
    "            for j in nodes:\n",
    "                submission_file[f'{dataset_id}_{i}_{j}'] = int(G.loc[i, j])\n",
    "\n",
    "    submission_file = pd.Series(submission_file)\n",
    "    submission_file = submission_file.reset_index()\n",
    "    submission_file.columns = [id_column_name, prediction_column_name]\n",
    "\n",
    "    return submission_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fceff69-b855-4146-8c57-b32d0ecb666e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m12:13:12\u001b[0m \u001b[33mno forbidden library found\u001b[0m\n",
      "\u001b[32m12:13:12\u001b[0m \u001b[33m\u001b[0m\n",
      "\u001b[32m12:13:12\u001b[0m started\n",
      "\u001b[32m12:13:12\u001b[0m running local test\n",
      "\u001b[32m12:13:12\u001b[0m \u001b[33minternet access isn't restricted, no check will be done\u001b[0m\n",
      "\u001b[32m12:13:12\u001b[0m \n",
      "\u001b[32m12:13:13\u001b[0m starting dag process...\n",
      "\u001b[32m12:13:13\u001b[0m \u001b[33mcall: train\u001b[0m\n",
      "\u001b[32m12:13:13\u001b[0m \u001b[33mcall: infer\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download data\\X_train.pickle from https://datacrunch-com.s3.eu-west-1.amazonaws.com/development/adia-tournament/data-releases/32/X_train.pickle (7591133 bytes)\n",
      "already exists: file length match\n",
      "download data\\y_train.pickle from https://datacrunch-com.s3.eu-west-1.amazonaws.com/development/adia-tournament/data-releases/32/y_train.pickle (98523 bytes)\n",
      "already exists: file length match\n",
      "download data\\X_test.pickle from https://datacrunch-com.s3.eu-west-1.amazonaws.com/development/adia-tournament/data-releases/32/X_test_reduced.pickle (329528 bytes)\n",
      "already exists: file length match\n",
      "download data\\y_test.pickle from https://datacrunch-com.s3.eu-west-1.amazonaws.com/development/adia-tournament/data-releases/32/y_test_reduced.pickle (4935 bytes)\n",
      "already exists: file length match\n",
      "download data\\example_prediction.parquet from https://datacrunch-com.s3.eu-west-1.amazonaws.com/development/adia-tournament/data-releases/32/example_prediction_reduced.parquet (3939 bytes)\n",
      "already exists: file length match\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0397\n",
      "0399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 12.67it/s]\n",
      "\u001b[32m12:13:13\u001b[0m \u001b[33msave prediction - path=data\\prediction.csv\u001b[0m\n",
      "\u001b[32m12:13:13\u001b[0m ended\n",
      "\u001b[32m12:13:13\u001b[0m \u001b[33mduration - time=00:00:01\u001b[0m\n",
      "\u001b[32m12:13:13\u001b[0m \u001b[33mmemory - before=\"334.20 MB\" after=\"339.89 MB\" consumed=\"5.69 MB\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0401\n",
      "0405\n",
      "0408\n"
     ]
    }
   ],
   "source": [
    "crunch.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
